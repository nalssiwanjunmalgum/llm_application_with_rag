import streamlit as st

from dotenv import load_dotenv
from langchain_openai import OpenAIEmbeddings
from langchain_pinecone import Pinecone
from langchain_openai import ChatOpenAI
from langsmith import Client
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate

from langchain.chains.retrieval import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain

load_dotenv()

st.set_page_config(page_title= "ì†Œë“ì„¸ ì±—ë´‡", page_icon="ğŸ¤–")

st.title("ğŸ¤– ì†Œë“ì„¸ ì±—ë´‡")
st.caption("ì†Œë“ì„¸ì— ê´€ë ¨ëœ ëª¨ë“  ê²ƒë“¤ì„ ì•Œë ¤ë“œë¦½ë‹ˆë‹¤!")

if 'message_list' not in st.session_state:
    st.session_state.message_list = []
# := ì˜ ì—­í• ì„ ì•Œì•„ì•¼ í•  ê²ƒ ê°™ë‹¤
print(f'before == {st.session_state.message_list}')
for msg in st.session_state.message_list:
    with st.chat_message(msg['role']):
        st.write(msg['content'])

def get_ai_message(user_message):

    embedding = OpenAIEmbeddings(model='text-embedding-3-large')
    index_name = 'tax-markdown-index'
    database = Pinecone.from_existing_index(index_name=index_name, embedding=embedding)

    llm = ChatOpenAI(model='gpt-4o')
    client = Client()
    hub_prompt = client.pull_prompt("rlm/rag-prompt")  # ê³µê°œ í”„ë¡¬í”„íŠ¸ë©´ owner í¬í•¨
    retriever = database.as_retriever(search_kwargs={'k':4})

    qa_chain = retrieval_qa.create_chain(
        llm=llm,
        retriever=retriever,
        chain_type="stuff",
        chain_type_kwargs={"prompt": hub_prompt},  # ì´ì œ contextë¥¼ ë°›ìŒ
        return_source_documents=False,             # ì›í•˜ë©´ ì¶œì²˜ë„ ë°˜í™˜
    )

    dictionary = ["ì‚¬ëŒì„ ë‚˜íƒ€ë‚´ëŠ” í‘œí˜„ -> ê±°ì£¼ì"]

    prompt = ChatPromptTemplate.from_template(f"""
        ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë³´ê³ , ìš°ë¦¬ì˜ ì‚¬ì „ì„ ì°¸ê³ í•´ì„œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë³€ê²½í•´ì£¼ì„¸ìš”.
        ë§Œì•½ ë³€ê²½í•  í•„ìš”ê°€ ì—†ë‹¤ê³  íŒë‹¨ëœë‹¤ë©´, ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë³€ê²½í•˜ì§€ ì•Šì•„ë„ ë©ë‹ˆë‹¤.
        ê·¸ëŸ° ê²½ìš°ì—ëŠ” ì§ˆë¬¸ë§Œ ë¦¬í„´í•´ì£¼ì„¸ìš”
        ì‚¬ì „: {dictionary}
        
        ì§ˆë¬¸: {{question}}
    """)

    dictionary_chain = prompt | llm | StrOutputParser()
    tax_chain = {"query": dictionary_chain} | qa_chain
    ai_message = tax_chain.invoke({"question" : user_message})

    return ai_message

if user_question := st.chat_input(placeholder='ì†Œë“ì„¸ ê´€ë ¨ ê¶ê¸ˆí•œ ë‚´ìš©ë“¤ì„ ë§ì”€í•´ì£¼ì„¸ìš”!'):
    with st.chat_message("user"):
        st.write(user_question)
    st.session_state.message_list.append({"role":"user", "content":user_question})

    ai_message = get_ai_message(user_question)

    with st.chat_message("ai"):
        st.write(ai_message)
    st.session_state.message_list.append({"role":"ai", "content": ai_message})

print(f'after == {st.session_state.message_list}')